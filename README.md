# ğŸ† LoRA å¾®èª¿ï¼šé‹å­é˜²éœ‰å°ˆå®¶ AI

## ğŸ“– å°ˆæ¡ˆç°¡ä»‹
æœ¬å°ˆæ¡ˆä½¿ç”¨ **LoRA (Low-Rank Adaptation)** æŠ€è¡“å° **èªè¨€æ¨¡å‹** é€²è¡Œå¾®èª¿ï¼Œå°ˆæ³¨æ–¼ **é‹å­é˜²éœ‰çŸ¥è­˜**ï¼Œä¸¦æ¶µè“‹ï¼š
- **é˜²éœ‰æ©Ÿåˆ¶**
- **é‹å­æè³ªå½±éŸ¿**
- **æ°£å‘³å½¢æˆ**
- **å¥åº·å½±éŸ¿**
- **é‹å­é˜²æ»‘èˆ‡æ€§èƒ½è®ŠåŒ–**

ğŸ“Œ **å°ˆæ¡ˆç›®æ¨™**
- æå‡ AI åœ¨ **é‹å­é˜²éœ‰é ˜åŸŸçš„æº–ç¢ºæ€§**ã€‚
- å„ªåŒ– AI **å›ç­”çš„æµæš¢åº¦èˆ‡å°ˆæ¥­åº¦**ã€‚
- **ç¸®çŸ­å›æ‡‰æ™‚é–“**ï¼Œæé«˜æ¨¡å‹æ¨ç†æ•ˆç‡ã€‚

---

## ğŸ“Š LoRA å¾®èª¿æ•ˆæœæ¯”è¼ƒ

| **è©•ä¼°æŒ‡æ¨™** | **LoRA å‰** | **LoRA å¾Œ** | **æå‡å¹…åº¦ (%)** |
|------------|------------|------------|------------|
| **å…§å®¹å®Œæ•´æ€§** | 5.4 | 7.2 | +33% |
| **é—œéµæ•¸æ“šæº–ç¢ºåº¦** | 4.9 | 6.6 | +35% |
| **èªæ„ä¸€è‡´æ€§** | 6.1 | 8.4 | +38% |
| **ç­”æ¡ˆå¯è®€æ€§** | 6.0 | 8.8 | +47% |
| **å›æ‡‰æ™‚é–“ (ç§’)** | 16.4s | 8.4s | **æå‡ 49%** |

âœ… **LoRA å¾®èª¿å¾Œçš„ AI åœ¨æº–ç¢ºæ€§ã€èªæ„ä¸€è‡´æ€§ã€å¯è®€æ€§ã€æ¨ç†é€Ÿåº¦æ–¹é¢å‡æœ‰é¡¯è‘—æå‡ï¼**

---

## ğŸ”§ è¨“ç·´ç´°ç¯€èˆ‡åƒæ•¸è¨­å®š

```python
from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported
# from transformers import EarlyStoppingCallback

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
            per_device_train_batch_size = 8,  # è®“æ¨¡å‹å­¸æ›´å¤š
            gradient_accumulation_steps = 2,  # æ›´é »ç¹æ›´æ–°æ¬Šé‡
            num_train_epochs = 13,  # è®“æ•¸æ“šå­¸ 13 æ¬¡
            max_steps = -1,  # è®“å®ƒä¾ç…§ epochs è¨ˆç®—æ­¥æ•¸
            learning_rate = 1.5e-5,  # é™ä½å­¸ç¿’ç‡ï¼Œè®“å­¸ç¿’æ›´ç´°ç·»
            lr_scheduler_type = "cosine_with_restarts",  # è®“å­¸ç¿’ç‡ä¸‹é™æ›´å¹³æ»‘
            warmup_steps = 5,
            fp16 = not is_bfloat16_supported(),
            bf16 = is_bfloat16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            seed = 8888,
            output_dir = "outputs",
            report_to = "none",
            # save_strategy = "epoch",  # æ¯å€‹ epoch å„²å­˜ checkpoint
            # evaluation_strategy = "epoch",  # æ¯å€‹ epoch è©•ä¼°
            # load_best_model_at_end = True,  # è¨“ç·´çµæŸæ™‚è¼‰å…¥æœ€ä½³æ¨¡å‹
            # metric_for_best_model = "loss",  # ç›£æ§ loss ä¾†æ±ºå®šæœ€ä½³æ¨¡å‹
        ),
    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # å¦‚æœ 3 å€‹ epoch æ²’æœ‰é€²æ­¥ï¼Œ
    )
